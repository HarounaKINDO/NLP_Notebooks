{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.6 MB 3.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/11.6 MB 3.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.9/11.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.5/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.1/11.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 3.4 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.1 MB 3.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.4/11.1 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.9/11.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.5/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.3 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.58.0-cp310-cp310-win_amd64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\syst\\downloads\\nlp\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.8/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.6/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.9/8.1 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/8.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.1 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.2 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4813dacf4c8943c95690579f33637d1bffda1aa1"
   },
   "source": [
    "# Simply about Word2Vec (Quora dataset)\n",
    "\n",
    "***Liana Napalkova***\n",
    "\n",
    "***21 October 2018***\n",
    "\n",
    "## Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [What is Word2Vec?](#word2vec)\n",
    "3. [Training of Word2Vec](#training)\n",
    "4. [Exploring the trained Word2Vec model](#exploring)\n",
    "5. [Playing with the trained Word2Vec model ](#playing)\n",
    "6. [When can we use Word2Vec?](#usage)\n",
    "\n",
    "**I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated.**\n",
    "\n",
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "If you have never used Word2Vec, then this notebook is for you! Here I will apply Word2Vec to the questions asked in Quora. The goal of the notebook is to better understand how Word2Vec works. Basically I wanted to resolve my personal doubts regarding Word2Vec. I will be very glad if my small analysis would be useful for someone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SYST\\AppData\\Local\\Temp\\ipykernel_5400\\1610249523.py:1: DtypeWarning: Columns (7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"q_quora.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2 is_duplicate Unnamed: 6  \\\n",
       "0  What is the step by step guide to invest in sh...            0        NaN   \n",
       "1  What would happen if the Indian government sto...            0        NaN   \n",
       "2  How can Internet speed be increased by hacking...            0        NaN   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...            0        NaN   \n",
       "4            Which fish would survive in salt water?            0        NaN   \n",
       "\n",
       "  Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  Unnamed: 12  \n",
       "0        NaN        NaN        NaN         NaN         NaN          NaN  \n",
       "1        NaN        NaN        NaN         NaN         NaN          NaN  \n",
       "2        NaN        NaN        NaN         NaN         NaN          NaN  \n",
       "3        NaN        NaN        NaN         NaN         NaN          NaN  \n",
       "4        NaN        NaN        NaN         NaN         NaN          NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"q_quora.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "525c4930df130c8078a576c6a05897bb6334df39",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404351, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba9b0f86cc7b16015820d2c1649f2355bf3066bd"
   },
   "source": [
    "The Quora dataset consists of 404.290 rows and 6 columns. To reach the goal of this notebook, I am only interested in columns that contain questions. These columns are \"question1\" and \"question2\". To reduce training time, I will only use the column \"question1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f737643d9fc4ed5310e424d3c0a232f44682370"
   },
   "source": [
    "## What is Word2Vec? <a name=\"word2vec\"></a>\n",
    "\n",
    "Word2Vec is one of many different word embedding techniques. In turn, word embedding is one of the most popular representation of document vocabulary. Thus, we get the following hierarchy:\n",
    "\n",
    "* Document vocabulary representation -> Word embedding -> Word2Vec\n",
    "\n",
    "So, what is word embedding? **Word embedding** is a vector representation of a word. For example, let's consider the folling three phases: \n",
    "* Agama is a reptile.\n",
    "* Snake is a reptile.\n",
    "* Reptile is a cold-blooded animal.\n",
    "\n",
    "The **dictionary** of this corpus of text (three phrases) consists of all unique words:\n",
    "> [\"agama\", \"reptile\", \"snake\", \"a\", \"is\", \"cold-blooded\", \"animal\"]\n",
    "\n",
    "A simple vector representation of the word \"agama\" would be:\n",
    "> [1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "where 1 stands for the word \"agama\" in our dictionary. This word embedding is called one-hot encoding. For any word, the length of a vector is always 7 in our example, because the dictionary consists of 7 unique words. This word embedding technique is very simple, but has a tremendous disadvantage. Let's take vector representations of two words: \n",
    "> \"agama\" and \"reptile\" -> [1, 0, 0, 0, 0, 0, 0] and [0, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "These two vectors provide no useful information regarding the relationships that may exist between the \"agama\" and \"reptile\". In other words, the similarity between words is not captured by the one-hot encoding. Loosely speaking, one-hot encoding does not enable the system making conclusions about the word \"agama\" given the word \"reptile\" (for example, such that agama is a cold-blooded animal).\n",
    "\n",
    "**Word2Vec is a more sophisticated word embedding technique**. This technique is based on the idea that words that occur in the same **contexts** tend to have similar meanings. I like this definition from Wikipedia \n",
    "\n",
    "> Word2Vec is a **two-layer neural network** that takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space (Wikipedia).\n",
    "\n",
    "Below I provide one of the best graphic representations of Word2Vec's word embedding that I found on Internet. This picture gives an intuition about how word embedding looks like in Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01271c4f209df29e4016ff4b58b0f074a75e6b01"
   },
   "source": [
    "![](http://adriancolyer.files.wordpress.com/2016/04/word2vec-distributed-representation.png?w=566&zoom=2)\n",
    "\n",
    "Ref: [The amazing power of word vectors](http://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8aedf4c46f5d9fe634e2c97a95450e065ae73e43"
   },
   "source": [
    "**Word2Vec itself is not a deep neural network**, but it turns input text into a numerical form that deep neural networks can process as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2344f77a4829a439379488785baa6298b093be5b"
   },
   "source": [
    "## Training of Word2Vec <a name=\"training\"></a>\n",
    "\n",
    "To train the Word2Vec two-layer neural network, the only thing that we need to do is to change the format of Quora dataset. In particular, I will transform the column \"question1\" into a list of lists. I will use the function \"simple_preprocess\" of GenSim package in order to transform each question (row) into a set of tokens, for example: \"what is your name\" -> \"what\", \"is\", \"your\", \"name\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27f155fbb44968d2d10e003ec2752ad9580be345",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_questions(row,column_name):\n",
    "    return gensim.utils.simple_preprocess(str(row[column_name]).encode('utf-8'))\n",
    "    \n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    documents.append(read_questions(row,\"question1\"))\n",
    "    if row[\"is_duplicate\"] == 0:\n",
    "        documents.append(read_questions(row,\"question2\"))1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_questions(row, column_name):\n",
    "    return gensim.utils.simple_preprocess(str(row[column_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    if pd.notnull(row[\"question1\"]):  # Vérifiez que la question n'est pas NaN\n",
    "        documents.append(read_questions(row, \"question1\"))\n",
    "    if row[\"is_duplicate\"] == 0 and pd.notnull(row[\"question2\"]):  # Vérifiez aussi pour question2\n",
    "        documents.append(read_questions(row, \"question2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'is', 'the', 'story', 'of', 'kohinoor', 'koh', 'noor', 'diamond']]\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))  # Doit être une liste\n",
    "print(type(documents[0]))  # Doit être une liste\n",
    "print(documents[:2])  # Affiche les deux premières entrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "334796b667faec66552cef5727ee0d8eb83642a1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of lists. Let's confirm:  <class 'list'>  of  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"List of lists. Let's confirm: \", type(documents), \" of \", type(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c3a7d8985f51334f729ca206a744160455267d4"
   },
   "source": [
    "Now Word2Vec will retrieve all unique words from all sub-lists of \"documents\", thereby constructing the vocabulary (unique words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "0d1956af61059aa4ddf611d1924600c67d3431c6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remplacez 'size' par 'vector_size'\n",
    "model = gensim.models.Word2Vec(vector_size=150, window=10, min_count=2, sg=1, workers=10)\n",
    "model.build_vocab(documents)  # Préparer le vocabulaire du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "981d5e8ac087a30294810a18513e3626e2e43f9a"
   },
   "source": [
    "Below I provide the definition of four parameters that we used to define a Word2Vec model:\n",
    "\n",
    "* **size:** The size means the dimensionality of word vectors. It defines the number of tokens used to represent each word. For example, rake a look at the picture above. The size would be equal to 4 in this example. Each input word would be represented by 4 tokens: King, Queen, Women, Princess.  Rule-of-thumb: If a dataset is small, then size should be small too. If a dataset is large, then size should be greater too. It's the question of tuning.\n",
    "\n",
    "* **window:** The maximum distance between the target word and its neighboring word. For example, let's take the phrase \"agama is a reptile \" with 4 words (suppose that we do not exclude the stop words). If window size is 2, then the vector of word \"agama\" is directly affected by the word \"is\" and \"a\". Rule-of-thumb: a smaller window should provide terms that are more related (of course, the exclusion of stop words should be considered).\n",
    "\n",
    "* **min_count:** Ignores all words with total frequency lower than this. For example, if the word frequency is extremally low, then this word might be considered as unimportant.\n",
    "\n",
    "* **sg:** Selects training algorithm: 1 for Skip-Gram; 0 for CBOW (Continuous Bag of Words).\n",
    "\n",
    "* **workers:** The number of worker threads used to train the model.\n",
    "\n",
    "More details about input parameters can be found [here](http://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96ce87e5ea042e98cc1bbb58f8f0c87d8711da2c"
   },
   "source": [
    "Once the vocabulary is created and the Word2Vec model is specified, I will train this model by calling \"train\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "6d7629860aa0e7c9be43af0d24317e9d046fd339",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SYST\\Downloads\\NLP\\venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1046\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_training_sanity(epochs\u001b[38;5;241m=\u001b[39mepochs, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words)\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_corpus_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss \u001b[38;5;241m=\u001b[39m compute_loss\n",
      "File \u001b[1;32mc:\\Users\\SYST\\Downloads\\NLP\\venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1497\u001b[0m, in \u001b[0;36mWord2Vec._check_corpus_sanity\u001b[1;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither one of corpus_file or corpus_iterable value must be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth corpus_file and corpus_iterable must not be provided at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train(sentences=documents, total_examples=len(documents), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98c4846760e19d393bd7b2429c4ca3b711a921ae"
   },
   "source": [
    "Let's see what happens when the function \"train\" is executed. I found a diagram (see below) that shows a Skip-Gram model architecture to produce a distributed representation of words. To understand this diagram, we should consider the size of vocablurary and the dimensionality of the word vectors (the parameter \"size\"). In our example, these are equal to 47.366 words and 150 tokens, respectively. If we take the word “immigration”, it will be one of the words in the 47.366 word vocabulary.  Therefore we can initially represent the word \"immigration\" as a 47.366 length one-hot vector.  We then link this input vector to a 150 node hidden layer.  There are totally 150 weights connecting the input layer and the hidden layer.  The activations of the nodes in this hidden layer are simply linear summations of the weighted inputs, e.g. `ohe*w1+ohe*w2+...+ohe*w150`.  These nodes are then fed into a softmax output layer.  During training, we want to change the weights of this neural network so that words surrounding “immigration” have a higher probability in the softmax output layer.  So, for instance, if our text data set has a lot of questions related to the immigration in Canada, we would want our network to assign large probabilities to words like “Canada”, \"Australia\"  (given lots of sentences containing “the immigration to Canada or Australia”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c8e4f4a9b33fb78087fd9f2bfa652b5c66dbd77"
   },
   "source": [
    "![](https://preview.ibb.co/iuzji0/Word2-Vec-skip-gram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1644a2fae2378a6bf4801bdca21c16ac4fde441"
   },
   "source": [
    "We finished training our Word2Vec model. What's next? The funny thing is that next we abandon the softmax layer and just use the 47.366 x 150 weight matrix as our word embedding lookup table. Sounds weird, isn't it? The only goal was to get the weights of the hidden layer. These weights are essentially the word embeddings that we had to learn. Once we know these word embeddings, we do not need the whole Word2Vec network anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7eb92af443a4bde0c37a6f75adb073be6a6c5fe3"
   },
   "source": [
    "## Exploring the trained Word2Vec model <a name=\"exploring\"></a>\n",
    "\n",
    "The learned vocabulary of tokens (words) is stored in \"model.wv.vocab\". In our case, the vocabulary includes 47.366 unique words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "7bd5364fdad14bf523c499a9b7ab479a952b3c13",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\n\u001b[0;32m      2\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(word)\n",
      "File \u001b[1;32mc:\\Users\\SYST\\Downloads\\NLP\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse KeyedVector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "count = 0\n",
    "for word in word_vectors.vocab:\n",
    "    if count<10:\n",
    "        print(word)\n",
    "        count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31360854e9228b62cedf18314415d410c26016af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a45dc4f286e27b7449a45be4d5d2442833cdc987"
   },
   "source": [
    "The embedded vectors for a specific token are stored in a KeyedVectors instance in \"model.wv\". We can see that the length of a word vector is equal to 150 as we defined by the parameter \"size\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a57556a59cdf75e00c1cba571dc8b48f52daf2b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vector = model.wv[\"immigration\"]  # numpy vector of a word\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8ba927edf82d97cd52749462dfed9644a712bbe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e601b7b57c45004b877a3059354481655a5da52"
   },
   "source": [
    "We can now create a two-dimensional semantic representation of word embeddings using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd7b0de2d192a07232fa1fc16dee300c753ad15d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wanted_words = []\n",
    "count = 0\n",
    "for word in word_vectors.vocab:\n",
    "    if count<150:\n",
    "        wanted_words.append(word)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "wanted_vocab = dict((k, word_vectors.vocab[k]) for k in wanted_words if k in word_vectors.vocab)\n",
    "wanted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f5225c8d5c3e8bca0035b9c67082883ed8f659c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = model[wanted_vocab] # X is an array of word vectors, each vector containing 150 tokens\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "Y = tsne_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0bb1c580c9d5b354965c5ddd279698c58716aa74",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Plot the t-SNE output\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.scatter(Y[:, 0], Y[:, 1])\n",
    "words = list(wanted_vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(Y[i, 0], Y[i, 1]))\n",
    "ax.set_yticklabels([]) #Hide ticks\n",
    "ax.set_xticklabels([]) #Hide ticks\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2bfa2eef6692a8a85614bcad7e6aa9df9e3a9e4"
   },
   "source": [
    "Not that bad. It's interesting to see that, for example, the word pairs \"motorola\" and \"phone\" or \"youtube\" and \"video\" are located quite close to each other. It means that these words appeared in the same context in the corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8ab644c772e8e4cf5310f10a145ef35ee83f2e3"
   },
   "source": [
    "## Playing with the trained Word2Vec model <a name=\"playing\"></a>\n",
    "\n",
    "Now we will see what we can actually do with our trained Word2Vec model. This overview does not pretend to be exhaustive. I will only check the most popular functionalities:\n",
    "* Given a word, find most similar words\n",
    "* Given words A and B, find other words that are similar to A and opposite to B\n",
    "* Given a sequence of words, find an odd word\n",
    "\n",
    "This first example shows a simple case of looking up top 5 words similar to the word \"phone\". To do this, I will call the \"most_similar\" function and provide the word \"phone\" as the positive example. This returns the top 5 similar words: \"mobile\", \"phones\", \"smartphone\", \"iphone\" and \"device\". \n",
    "\n",
    "You may notice that there is no word \"motorola\" in the list of top 5 similar words, while the word \"motorola\" appears in t-SNE graphic. Please remember that t-SNE was applied on a small subset of vocabulary (150 words). Therefore, the words \"mobile\", \"phones\", \"smartphone\", etc. were simply not included in these 150 words and therefore they have not appeared on the t-SNE graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "039de0546d5ba606b931f12a909f8d47112bcabb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "w1 = \"phone\"\n",
    "model.wv.most_similar(positive=w1, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cba8407b1f9bc34b6ff63735d30969da8385ce58"
   },
   "source": [
    "Next, given positive and negative words, we will find top 2 words that are similar to positivie words and opposite to negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78d2258bebde757c53fd015747e705b95e2c9655",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "w1 = [\"women\",\"rights\"]\n",
    "w2 = [\"girls\"]\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "136be2d17113e9593b0fa975658a9cc58b69efbf"
   },
   "source": [
    "Finally, I will use Word2Vec to find odd items given a list of items \"government\",\"corruption\" and \"peace\". For most of people this is a rhetorical question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6878e86a18d32c65dc22ac960781b2ae66e0df60",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"government\",\"corruption\",\"peace\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d92a21e3226d9423908b69a5055849263dc483e"
   },
   "source": [
    "Do you agree with the opinion of Word2Vec?:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec2e2a80dc6a5b1b5dcee7ba8ce4438fcce8cd73"
   },
   "source": [
    "\n",
    "## When can we use Word2Vec? <a name=\"usage\"></a>\n",
    "\n",
    "There are many application scenarios for Word2Vec: sentiment analysis, recommender systems, etc. For example, imagine that you have tags for a large amount of questions in Quora. Given a tag, you want to find similar tags in order to recommend them to a user for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "mon_env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
